
---

# Ejercicio 1

[![TP](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/C4570/AA2-Menescaldi-Britos/blob/main/Ejercicio_1.ipynb)

---

# Ejercicio 2: Sistema de Clasificaci√≥n de Gestos (Piedra, Papel o Tijeras)

Este ejercicio tiene como objetivo implementar un sistema para reconocer gestos de **piedra**, **papel** y **tijeras** utilizando **MediaPipe** y un modelo de red neuronal. El sistema consta de tres pasos: grabaci√≥n del dataset, entrenamiento del modelo, y ejecuci√≥n del clasificador en tiempo real.

### üî• Instrucciones para ejecutar el proyecto:

### 1. **Grabar el Dataset** (`record-dataset.py`)

En esta etapa, grabar√°s gestos de la mano para crear un dataset de entrenamiento.

1. **Ejecuta** el script `record-dataset.py`:
    ```bash
    python record-dataset.py
    ```

2. Aparecer√° la c√°mara y deber√°s realizar los gestos correspondientes a:
    - **ü™® Piedra**: Presiona la tecla `1`.
    - **‚úÇÔ∏è Tijeras**: Presiona la tecla `2`.
    - **üìÑ Papel**: Presiona la tecla `3`.

3. Cada vez que presiones uno de estos n√∫meros, el sistema capturar√° la posici√≥n de tu mano y etiquetar√° los datos correctamente. 

4. **Instrucciones flotantes** en pantalla te recordar√°n qu√© tecla presionar para cada gesto. Si deseas detener la grabaci√≥n, simplemente presiona la tecla **`q`**.

5. Al finalizar, los datos se guardar√°n autom√°ticamente en la carpeta **Ejercicio 2** en archivos `.npy`:
   - `rps_dataset.npy`: contiene los landmarks de los gestos grabados.
   - `rps_labels.npy`: contiene las etiquetas correspondientes (0 para piedra, 1 para papel, 2 para tijeras).

### 2. **Entrenar el Modelo** (`train-gesture-classifier.py`)

Con los datos capturados, entrenar√°s un modelo de red neuronal para clasificar los gestos.

1. **Ejecuta** el script `train-gesture-classifier.py`:
    ```bash
    python train-gesture-classifier.py
    ```

2. El script cargar√° el dataset generado en la etapa anterior (`rps_dataset.npy` y `rps_labels.npy`), y entrenar√° una red neuronal con estos datos.

3. El modelo entrenado se guardar√° autom√°ticamente en un archivo llamado `rps_model.h5`.

### 3. **Probar el Sistema Completo** (`rock-paper-scissors.py`)

Finalmente, pondr√°s a prueba el modelo entrenado para que clasifique gestos en tiempo real usando la c√°mara.

1. **Ejecuta** el script `rock-paper-scissors.py`:
    ```bash
    python rock-paper-scissors.py
    ```

2. La c√°mara se abrir√° nuevamente. Al realizar un gesto con la mano frente a la c√°mara, el sistema utilizar√° **MediaPipe** para detectar la mano y el modelo entrenado para predecir si est√°s haciendo **ü™® piedra**, **‚úÇÔ∏è tijeras** o **üìÑ papel**.

3. El gesto reconocido se mostrar√° en pantalla en tiempo real.

---

# Ejercicio 3

[![TP](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FCEIA-AAII/lab5/blob/main/Ejercicio_3.ipynb)

---

## Fuente de datos

Puedes acceder a los datos necesarios para estos ejercicios desde el siguiente enlace:

[Fuente de datos](https://drive.google.com/drive/folders/1ll10p_9Kvsq_PGI7eKPBdEaxUL3fx8Eb?usp=sharing)

---
